{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "96ca3b03-cf12-4e5e-8dfe-85d23d761c3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a9867524-cf86-4cff-b2ab-1eb37ba62a79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33m  DEPRECATION: Building 'multitasking' using the legacy setup.py bdist_wheel mechanism, which will be removed in a future version. pip 25.3 will enforce this behaviour change. A possible replacement is to use the standardized build interface by setting the `--use-pep517` option, (possibly combined with `--no-build-isolation`), or adding a `pyproject.toml` file to the source tree of 'multitasking'. Discussion can be found at https://github.com/pypa/pip/issues/6334\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n",
      "Ready: data/raw exists ✅\n"
     ]
    }
   ],
   "source": [
    "# Install needed libraries (safe to re-run)\n",
    "%pip install -q yfinance python-dotenv beautifulsoup4 lxml\n",
    "\n",
    "# Make sure the data/raw folder exists\n",
    "import os, pathlib\n",
    "pathlib.Path(\"../data/raw\").mkdir(parents=True, exist_ok=True)\n",
    "print(\"Ready: data/raw exists ✅\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a104fc81-643f-4fb3-a0f3-5f48620eaa74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation passed ✅\n",
      "Rows: 125 | Columns: [('Date', ''), ('Adj Close', 'AAPL'), ('Close', 'AAPL'), ('High', 'AAPL'), ('Low', 'AAPL'), ('Open', 'AAPL'), ('Volume', 'AAPL')]\n",
      "Saved to: /Users/ivysingal/bootcamp_ivy_singal/data/raw/api_yfinance_AAPL_20250820-2218.csv\n"
     ]
    }
   ],
   "source": [
    "# --- API INGESTION: yfinance fallback per homework ---\n",
    "# PDF asks: choose an endpoint/ticker, pull data (API or yfinance), parse types, validate, save to data/raw/:contentReference[oaicite:1]{index=1}\n",
    "\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "import yfinance as yf\n",
    "\n",
    "# 0) Load secrets if needed (safe even if you don't use any API keys here)\n",
    "load_dotenv()  # looks for a local .env (not committed to GitHub)\n",
    "\n",
    "# 1) Pick a ticker (you can change later if you want)\n",
    "TICKER = \"AAPL\"\n",
    "\n",
    "# 2) Pull recent daily data via yfinance (fallback allowed by the homework)\n",
    "df_api = yf.download(TICKER, period=\"6mo\", interval=\"1d\", auto_adjust=False, progress=False)\n",
    "\n",
    "# 3) Tidy: move Date out of index, ensure dtypes (dates/floats)\n",
    "df_api = df_api.reset_index()  # brings 'Date' out of the index\n",
    "df_api[\"Date\"] = pd.to_datetime(df_api[\"Date\"])  # parse to datetime\n",
    "\n",
    "# 4) Validate required columns, NA counts, and shape (simple rules)\n",
    "required_cols = [\"Date\", \"Open\", \"High\", \"Low\", \"Close\", \"Volume\"]\n",
    "missing = [c for c in required_cols if c not in df_api.columns]\n",
    "assert not missing, f\"Missing required columns: {missing}\"\n",
    "\n",
    "na_counts = df_api[required_cols].isna().sum()\n",
    "assert na_counts.sum() == 0, f\"Found NAs in required columns:\\n{na_counts}\"\n",
    "\n",
    "assert len(df_api) > 0, \"No rows returned from API.\"\n",
    "\n",
    "print(\"Validation passed ✅\")\n",
    "print(\"Rows:\", len(df_api), \"| Columns:\", list(df_api.columns))\n",
    "\n",
    "# 5) Save raw CSV to data/raw/ with timestamped, reproducible filename\n",
    "ts = datetime.now().strftime(\"%Y%m%d-%H%M\")\n",
    "out_path = Path(\"../data/raw\") / f\"api_yfinance_{TICKER}_{ts}.csv\"\n",
    "df_api.to_csv(out_path, index=False)\n",
    "print(\"Saved to:\", out_path.resolve())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cf740153-4428-49ab-9c6a-2ecf2a4f3b7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 5 rows:\n",
      "  Symbol             Security             GICS Sector  \\\n",
      "0    MMM                   3M             Industrials   \n",
      "1    AOS          A. O. Smith             Industrials   \n",
      "2    ABT  Abbott Laboratories             Health Care   \n",
      "3   ABBV               AbbVie             Health Care   \n",
      "4    ACN            Accenture  Information Technology   \n",
      "\n",
      "                GICS Sub-Industry    Headquarters Location  Date added  \\\n",
      "0        Industrial Conglomerates    Saint Paul, Minnesota  1957-03-04   \n",
      "1               Building Products     Milwaukee, Wisconsin  2017-07-26   \n",
      "2           Health Care Equipment  North Chicago, Illinois  1957-03-04   \n",
      "3                   Biotechnology  North Chicago, Illinois  2012-12-31   \n",
      "4  IT Consulting & Other Services          Dublin, Ireland  2011-07-06   \n",
      "\n",
      "       CIK      Founded  \n",
      "0    66740         1902  \n",
      "1    91142         1916  \n",
      "2     1800         1888  \n",
      "3  1551152  2013 (1888)  \n",
      "4  1467373         1989  \n",
      "Validation passed ✅\n",
      "Rows: 503 | Columns: ['Symbol', 'Security', 'GICS Sector', 'GICS Sub-Industry', 'Headquarters Location', 'Date added', 'CIK', 'Founded']\n",
      "Saved to: /Users/ivysingal/bootcamp_ivy_singal/data/raw/scraped_sp500_20250820-2219.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/sv/412p5qsn54n45884yn3r2yd00000gn/T/ipykernel_40850/4104906268.py:18: FutureWarning: Passing literal html to 'read_html' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
      "  df_scrape = pd.read_html(str(table))[0]\n"
     ]
    }
   ],
   "source": [
    "# --- WEB SCRAPING: small table from Wikipedia (per homework) ---\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "# 1) Target page with a nice table\n",
    "url = \"https://en.wikipedia.org/wiki/List_of_S%26P_500_companies\"\n",
    "\n",
    "resp = requests.get(url)\n",
    "soup = BeautifulSoup(resp.text, \"html.parser\")\n",
    "\n",
    "# 2) Find the first big wikitable\n",
    "table = soup.find(\"table\", {\"id\": \"constituents\"})\n",
    "\n",
    "# 3) Parse table into DataFrame\n",
    "df_scrape = pd.read_html(str(table))[0]\n",
    "\n",
    "# 4) Basic validation\n",
    "print(\"First 5 rows:\")\n",
    "print(df_scrape.head())\n",
    "\n",
    "# Ensure key columns exist\n",
    "required_cols = [\"Symbol\", \"Security\", \"GICS Sector\"]\n",
    "missing = [c for c in required_cols if c not in df_scrape.columns]\n",
    "assert not missing, f\"Missing required columns: {missing}\"\n",
    "\n",
    "# Drop NAs and check length\n",
    "df_scrape = df_scrape.dropna(subset=required_cols)\n",
    "assert len(df_scrape) > 0, \"Scraped DataFrame is empty!\"\n",
    "\n",
    "print(\"Validation passed ✅\")\n",
    "print(\"Rows:\", len(df_scrape), \"| Columns:\", list(df_scrape.columns))\n",
    "\n",
    "# 5) Save raw CSV to data/raw/\n",
    "ts = datetime.now().strftime(\"%Y%m%d-%H%M\")\n",
    "out_path = Path(\"../data/raw\") / f\"scraped_sp500_{ts}.csv\"\n",
    "df_scrape.to_csv(out_path, index=False)\n",
    "print(\"Saved to:\", out_path.resolve())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13a26a7d-5bbb-4d39-8d71-7c65884d7868",
   "metadata": {},
   "source": [
    "## Data Ingestion Documentation (Stage 04)\n",
    "\n",
    "**Sources & Endpoints**\n",
    "- API (fallback): yfinance daily prices for `AAPL` (last ~6 months).\n",
    "- Web scrape: Wikipedia “List of S&P 500 companies” (table id = `constituents`).\n",
    "\n",
    "**Parameters Used**\n",
    "- API: period = 6mo, interval = 1d, auto_adjust = False.\n",
    "- Scrape: single HTML table parsed with pandas.read_html over the `#constituents` table.\n",
    "\n",
    "**Validation Logic**\n",
    "- API: required columns = Date, Open, High, Low, Close, Volume; assert no NAs; assert >0 rows.\n",
    "- Scrape: required columns = Symbol, Security, GICS Sector; drop NA in required cols; assert >0 rows.\n",
    "\n",
    "**Saved Files**\n",
    "- `data/raw/api_yfinance_AAPL_<YYYYMMDD-HHMM>.csv`\n",
    "- `data/raw/scraped_sp500_<YYYYMMDD-HHMM>.csv`\n",
    "\n",
    "**.env & Reproducibility**\n",
    "- `.env` (secrets) kept local, **not committed**.\n",
    "- If an API requires a key, load via `dotenv` in code.\n",
    "- Notebook contains sources, params, and validation steps as required. :contentReference[oaicite:0]{index=0}\n",
    "\n",
    "**Assumptions & Risks**\n",
    "- Wikipedia structure can change (table id/columns).\n",
    "- Market data can have holidays/missing days.\n",
    "- If an API rate-limits or key expires, ingestion may fail; yfinance used as a permitted fallback."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "524ea6df-ac31-4dba-8e24-24e133f151ed",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
