{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "96ca3b03-cf12-4e5e-8dfe-85d23d761c3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a9867524-cf86-4cff-b2ab-1eb37ba62a79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33m  DEPRECATION: Building 'multitasking' using the legacy setup.py bdist_wheel mechanism, which will be removed in a future version. pip 25.3 will enforce this behaviour change. A possible replacement is to use the standardized build interface by setting the `--use-pep517` option, (possibly combined with `--no-build-isolation`), or adding a `pyproject.toml` file to the source tree of 'multitasking'. Discussion can be found at https://github.com/pypa/pip/issues/6334\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n",
      "Ready: data/raw exists ✅\n"
     ]
    }
   ],
   "source": [
    "# Install needed libraries (safe to re-run)\n",
    "%pip install -q yfinance python-dotenv beautifulsoup4 lxml\n",
    "\n",
    "# Make sure the data/raw folder exists\n",
    "import os, pathlib\n",
    "pathlib.Path(\"../data/raw\").mkdir(parents=True, exist_ok=True)\n",
    "print(\"Ready: data/raw exists ✅\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a104fc81-643f-4fb3-a0f3-5f48620eaa74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation passed ✅\n",
      "Rows: 125 | Columns: [('Date', ''), ('Adj Close', 'AAPL'), ('Close', 'AAPL'), ('High', 'AAPL'), ('Low', 'AAPL'), ('Open', 'AAPL'), ('Volume', 'AAPL')]\n",
      "Saved to: /Users/ivysingal/bootcamp_ivy_singal/data/raw/api_yfinance_AAPL_20250820-2218.csv\n"
     ]
    }
   ],
   "source": [
    "# --- API INGESTION: yfinance fallback per homework ---\n",
    "# PDF asks: choose an endpoint/ticker, pull data (API or yfinance), parse types, validate, save to data/raw/:contentReference[oaicite:1]{index=1}\n",
    "\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "import yfinance as yf\n",
    "\n",
    "# 0) Load secrets if needed (safe even if you don't use any API keys here)\n",
    "load_dotenv()  # looks for a local .env (not committed to GitHub)\n",
    "\n",
    "# 1) Pick a ticker (you can change later if you want)\n",
    "TICKER = \"AAPL\"\n",
    "\n",
    "# 2) Pull recent daily data via yfinance (fallback allowed by the homework)\n",
    "df_api = yf.download(TICKER, period=\"6mo\", interval=\"1d\", auto_adjust=False, progress=False)\n",
    "\n",
    "# 3) Tidy: move Date out of index, ensure dtypes (dates/floats)\n",
    "df_api = df_api.reset_index()  # brings 'Date' out of the index\n",
    "df_api[\"Date\"] = pd.to_datetime(df_api[\"Date\"])  # parse to datetime\n",
    "\n",
    "# 4) Validate required columns, NA counts, and shape (simple rules)\n",
    "required_cols = [\"Date\", \"Open\", \"High\", \"Low\", \"Close\", \"Volume\"]\n",
    "missing = [c for c in required_cols if c not in df_api.columns]\n",
    "assert not missing, f\"Missing required columns: {missing}\"\n",
    "\n",
    "na_counts = df_api[required_cols].isna().sum()\n",
    "assert na_counts.sum() == 0, f\"Found NAs in required columns:\\n{na_counts}\"\n",
    "\n",
    "assert len(df_api) > 0, \"No rows returned from API.\"\n",
    "\n",
    "print(\"Validation passed ✅\")\n",
    "print(\"Rows:\", len(df_api), \"| Columns:\", list(df_api.columns))\n",
    "\n",
    "# 5) Save raw CSV to data/raw/ with timestamped, reproducible filename\n",
    "ts = datetime.now().strftime(\"%Y%m%d-%H%M\")\n",
    "out_path = Path(\"../data/raw\") / f\"api_yfinance_{TICKER}_{ts}.csv\"\n",
    "df_api.to_csv(out_path, index=False)\n",
    "print(\"Saved to:\", out_path.resolve())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cf740153-4428-49ab-9c6a-2ecf2a4f3b7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 5 rows:\n",
      "  Symbol             Security             GICS Sector  \\\n",
      "0    MMM                   3M             Industrials   \n",
      "1    AOS          A. O. Smith             Industrials   \n",
      "2    ABT  Abbott Laboratories             Health Care   \n",
      "3   ABBV               AbbVie             Health Care   \n",
      "4    ACN            Accenture  Information Technology   \n",
      "\n",
      "                GICS Sub-Industry    Headquarters Location  Date added  \\\n",
      "0        Industrial Conglomerates    Saint Paul, Minnesota  1957-03-04   \n",
      "1               Building Products     Milwaukee, Wisconsin  2017-07-26   \n",
      "2           Health Care Equipment  North Chicago, Illinois  1957-03-04   \n",
      "3                   Biotechnology  North Chicago, Illinois  2012-12-31   \n",
      "4  IT Consulting & Other Services          Dublin, Ireland  2011-07-06   \n",
      "\n",
      "       CIK      Founded  \n",
      "0    66740         1902  \n",
      "1    91142         1916  \n",
      "2     1800         1888  \n",
      "3  1551152  2013 (1888)  \n",
      "4  1467373         1989  \n",
      "Validation passed ✅\n",
      "Rows: 503 | Columns: ['Symbol', 'Security', 'GICS Sector', 'GICS Sub-Industry', 'Headquarters Location', 'Date added', 'CIK', 'Founded']\n",
      "Saved to: /Users/ivysingal/bootcamp_ivy_singal/data/raw/scraped_sp500_20250820-2219.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/sv/412p5qsn54n45884yn3r2yd00000gn/T/ipykernel_40850/4104906268.py:18: FutureWarning: Passing literal html to 'read_html' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
      "  df_scrape = pd.read_html(str(table))[0]\n"
     ]
    }
   ],
   "source": [
    "# --- WEB SCRAPING: small table from Wikipedia (per homework) ---\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "# 1) Target page with a nice table\n",
    "url = \"https://en.wikipedia.org/wiki/List_of_S%26P_500_companies\"\n",
    "\n",
    "resp = requests.get(url)\n",
    "soup = BeautifulSoup(resp.text, \"html.parser\")\n",
    "\n",
    "# 2) Find the first big wikitable\n",
    "table = soup.find(\"table\", {\"id\": \"constituents\"})\n",
    "\n",
    "# 3) Parse table into DataFrame\n",
    "df_scrape = pd.read_html(str(table))[0]\n",
    "\n",
    "# 4) Basic validation\n",
    "print(\"First 5 rows:\")\n",
    "print(df_scrape.head())\n",
    "\n",
    "# Ensure key columns exist\n",
    "required_cols = [\"Symbol\", \"Security\", \"GICS Sector\"]\n",
    "missing = [c for c in required_cols if c not in df_scrape.columns]\n",
    "assert not missing, f\"Missing required columns: {missing}\"\n",
    "\n",
    "# Drop NAs and check length\n",
    "df_scrape = df_scrape.dropna(subset=required_cols)\n",
    "assert len(df_scrape) > 0, \"Scraped DataFrame is empty!\"\n",
    "\n",
    "print(\"Validation passed ✅\")\n",
    "print(\"Rows:\", len(df_scrape), \"| Columns:\", list(df_scrape.columns))\n",
    "\n",
    "# 5) Save raw CSV to data/raw/\n",
    "ts = datetime.now().strftime(\"%Y%m%d-%H%M\")\n",
    "out_path = Path(\"../data/raw\") / f\"scraped_sp500_{ts}.csv\"\n",
    "df_scrape.to_csv(out_path, index=False)\n",
    "print(\"Saved to:\", out_path.resolve())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13a26a7d-5bbb-4d39-8d71-7c65884d7868",
   "metadata": {},
   "source": [
    "## Data Ingestion Documentation (Stage 04)\n",
    "\n",
    "**Sources & Endpoints**\n",
    "- API (fallback): yfinance daily prices for `AAPL` (last ~6 months).\n",
    "- Web scrape: Wikipedia “List of S&P 500 companies” (table id = `constituents`).\n",
    "\n",
    "**Parameters Used**\n",
    "- API: period = 6mo, interval = 1d, auto_adjust = False.\n",
    "- Scrape: single HTML table parsed with pandas.read_html over the `#constituents` table.\n",
    "\n",
    "**Validation Logic**\n",
    "- API: required columns = Date, Open, High, Low, Close, Volume; assert no NAs; assert >0 rows.\n",
    "- Scrape: required columns = Symbol, Security, GICS Sector; drop NA in required cols; assert >0 rows.\n",
    "\n",
    "**Saved Files**\n",
    "- `data/raw/api_yfinance_AAPL_<YYYYMMDD-HHMM>.csv`\n",
    "- `data/raw/scraped_sp500_<YYYYMMDD-HHMM>.csv`\n",
    "\n",
    "**.env & Reproducibility**\n",
    "- `.env` (secrets) kept local, **not committed**.\n",
    "- If an API requires a key, load via `dotenv` in code.\n",
    "- Notebook contains sources, params, and validation steps as required. :contentReference[oaicite:0]{index=0}\n",
    "\n",
    "**Assumptions & Risks**\n",
    "- Wikipedia structure can change (table id/columns).\n",
    "- Market data can have holidays/missing days.\n",
    "- If an API rate-limits or key expires, ingestion may fail; yfinance used as a permitted fallback."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "524ea6df-ac31-4dba-8e24-24e133f151ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/raw folder: /Users/ivysingal/bootcamp_ivy_singal/data/raw\n",
      "API CSVs:    ['api_yfinance_AAPL_20250820-2218.csv']\n",
      "Scrape CSVs: ['scraped_sp500_20250820-2219.csv']\n",
      "\n",
      "Checks →  API file present?: True | Scrape file present?: True\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "raw_dir = Path(\"../data/raw\")\n",
    "api_files    = sorted(raw_dir.glob(\"api_yfinance_*_*.csv\"))\n",
    "scrape_files = sorted(raw_dir.glob(\"scraped_sp500_*.csv\"))\n",
    "\n",
    "print(\"data/raw folder:\", raw_dir.resolve())\n",
    "print(\"API CSVs:   \", [p.name for p in api_files][-3:])\n",
    "print(\"Scrape CSVs:\", [p.name for p in scrape_files][-3:])\n",
    "\n",
    "API_OK    = len(api_files) > 0\n",
    "SCRAPE_OK = len(scrape_files) > 0\n",
    "print(\"\\nChecks →  API file present?:\", API_OK, \"| Scrape file present?:\", SCRAPE_OK)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "02fe4519-9c12-42ba-9e55-ee7cfcb75104",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NA counts per required col:\n",
      "Date      1\n",
      "Open      0\n",
      "High      0\n",
      "Low       0\n",
      "Close     0\n",
      "Volume    0\n",
      "dtype: int64\n",
      "\n",
      "Last 5 rows (often where NAs show up):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Adj Close</th>\n",
       "      <th>Close</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Open</th>\n",
       "      <th>Volume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>2025-08-14</td>\n",
       "      <td>232.77999877929688</td>\n",
       "      <td>232.77999877929688</td>\n",
       "      <td>235.1199951171875</td>\n",
       "      <td>230.85000610351562</td>\n",
       "      <td>234.05999755859375</td>\n",
       "      <td>51916300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>2025-08-15</td>\n",
       "      <td>231.58999633789062</td>\n",
       "      <td>231.58999633789062</td>\n",
       "      <td>234.27999877929688</td>\n",
       "      <td>229.33999633789062</td>\n",
       "      <td>234.0</td>\n",
       "      <td>56038700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>2025-08-18</td>\n",
       "      <td>230.88999938964844</td>\n",
       "      <td>230.88999938964844</td>\n",
       "      <td>233.1199951171875</td>\n",
       "      <td>230.11000061035156</td>\n",
       "      <td>231.6999969482422</td>\n",
       "      <td>37476200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>2025-08-19</td>\n",
       "      <td>230.55999755859375</td>\n",
       "      <td>230.55999755859375</td>\n",
       "      <td>232.8699951171875</td>\n",
       "      <td>229.35000610351562</td>\n",
       "      <td>231.27999877929688</td>\n",
       "      <td>39402600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>2025-08-20</td>\n",
       "      <td>226.00999450683594</td>\n",
       "      <td>226.00999450683594</td>\n",
       "      <td>230.47000122070312</td>\n",
       "      <td>225.77000427246094</td>\n",
       "      <td>229.97999572753906</td>\n",
       "      <td>42190600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Date           Adj Close               Close                High  \\\n",
       "121 2025-08-14  232.77999877929688  232.77999877929688   235.1199951171875   \n",
       "122 2025-08-15  231.58999633789062  231.58999633789062  234.27999877929688   \n",
       "123 2025-08-18  230.88999938964844  230.88999938964844   233.1199951171875   \n",
       "124 2025-08-19  230.55999755859375  230.55999755859375   232.8699951171875   \n",
       "125 2025-08-20  226.00999450683594  226.00999450683594  230.47000122070312   \n",
       "\n",
       "                    Low                Open    Volume  \n",
       "121  230.85000610351562  234.05999755859375  51916300  \n",
       "122  229.33999633789062               234.0  56038700  \n",
       "123  230.11000061035156   231.6999969482422  37476200  \n",
       "124  229.35000610351562  231.27999877929688  39402600  \n",
       "125  225.77000427246094  229.97999572753906  42190600  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Run this where your validation failed\n",
    "print(\"NA counts per required col:\")\n",
    "print(api[req_api].isna().sum())\n",
    "\n",
    "print(\"\\nLast 5 rows (often where NAs show up):\")\n",
    "display(api.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f222f088-489c-47da-9dcf-657137cf0047",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved cleaned raw to: /Users/ivysingal/bootcamp_ivy_singal/data/raw/api_yfinance_AAPL_20250820-2245_clean.csv | Rows: 125\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "# 1) Drop rows that have any NA in required columns\n",
    "api_clean = api.dropna(subset=req_api).copy()\n",
    "\n",
    "# (Optional) Also ensure dtypes are correct floats/ints\n",
    "float_cols = [\"Open\",\"High\",\"Low\",\"Close\"]\n",
    "for c in float_cols:\n",
    "    api_clean[c] = api_clean[c].astype(float)\n",
    "api_clean[\"Volume\"] = api_clean[\"Volume\"].astype(\"int64\", errors=\"ignore\")\n",
    "\n",
    "# 2) Save a new raw file (timestamped)\n",
    "ts = datetime.now().strftime(\"%Y%m%d-%H%M\")\n",
    "out_path_clean = Path(\"../data/raw\") / f\"api_yfinance_AAPL_{ts}_clean.csv\"\n",
    "api_clean.to_csv(out_path_clean, index=False)\n",
    "print(\"Saved cleaned raw to:\", out_path_clean.resolve(), \"| Rows:\", len(api_clean))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0364e439-5dc0-4d93-9009-cbfa249ffcb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "API file: api_yfinance_AAPL_20250820-2245_clean.csv\n",
      "Shape: (125, 7)\n",
      "\n",
      "NA check (required cols):\n",
      "Date      0\n",
      "Open      0\n",
      "High      0\n",
      "Low       0\n",
      "Close     0\n",
      "Volume    0\n",
      "dtype: int64\n",
      "\n",
      "API validation passed ✅\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "api_path = sorted(Path(\"../data/raw\").glob(\"api_yfinance_AAPL_*_clean.csv\"))[-1]\n",
    "api = pd.read_csv(api_path, parse_dates=[\"Date\"])\n",
    "\n",
    "print(\"API file:\", api_path.name)\n",
    "print(\"Shape:\", api.shape)\n",
    "print(\"\\nNA check (required cols):\")\n",
    "print(api[req_api].isna().sum())\n",
    "\n",
    "assert api.shape[0] > 0, \"API data is empty\"\n",
    "missing = [c for c in req_api if c not in api.columns]\n",
    "assert not missing, f\"Missing columns: {missing}\"\n",
    "assert api[req_api].isna().sum().sum() == 0, \"NAs found in required columns\"\n",
    "print(\"\\nAPI validation passed ✅\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b7ac4c5d-5a22-4447-8d37-c1b2e161d950",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved scrape file: ../data/raw/scrape_2025-08-20_22-47-19.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/sv/412p5qsn54n45884yn3r2yd00000gn/T/ipykernel_41212/3147818946.py:12: FutureWarning: Passing literal html to 'read_html' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
      "  df_scrape = pd.read_html(str(table))[0]\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime\n",
    "\n",
    "# --- Scrape again ---\n",
    "url = \"https://en.wikipedia.org/wiki/List_of_S%26P_500_companies\"\n",
    "html = requests.get(url).content\n",
    "soup = BeautifulSoup(html, \"html.parser\")\n",
    "table = soup.find(\"table\")\n",
    "\n",
    "df_scrape = pd.read_html(str(table))[0]\n",
    "\n",
    "# --- Save scrape to CSV ---\n",
    "scrape_path = f\"../data/raw/scrape_{datetime.now().strftime('%Y-%m-%d_%H-%M-%S')}.csv\"\n",
    "df_scrape.to_csv(scrape_path, index=False)\n",
    "print(\"Saved scrape file:\", scrape_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "27f3c3fe-10b6-4e86-a3bd-b2f6f41a9060",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scrape file: scrape_2025-08-20_22-47-19.csv\n",
      "\n",
      "Scrape validation passed ✅\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "req_scrape = [\"Symbol\", \"Security\", \"GICS Sector\", \"GICS Sub-Industry\"]\n",
    "scrape_path = sorted(Path(\"../data/raw\").glob(\"scrape_*.csv\"))[-1]\n",
    "scrape = pd.read_csv(scrape_path)\n",
    "\n",
    "print(\"Scrape file:\", scrape_path.name)\n",
    "\n",
    "# Validation checks\n",
    "missing = [c for c in req_scrape if c not in scrape.columns]\n",
    "assert scrape.shape[0] > 0, \"Scrape data is empty\"\n",
    "assert not missing, f\"Missing columns: {missing}\"\n",
    "assert scrape[req_scrape].isna().sum().sum() == 0, \"NAs found in required columns\"\n",
    "print(\"\\nScrape validation passed ✅\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78015f6c-a61e-4da5-8e18-c1aa32f1f678",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
