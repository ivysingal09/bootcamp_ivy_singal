# Bootcamp Repository

## Folder Structure
- **homework/** â†’ All homework contributions go here.
- **project/** â†’ All project contributions go here.
- **class_materials/** â†’ Local files only, not pushed to GitHub.

## Homework Rules
- Each homework must be in its own folder (homework0, homework1, ...).
- Include all required files.

## Project Rules
- Keep files organized and clearly named.
<<<<<<< HEAD

# Bootcamp Homework â€“ Ivy Singal  

This repository contains my homework submissions for the Bootcamp.  
It is structured to keep code, data, notebooks, and reports organized.

---

## ðŸ“‚ Repository Layout  
- **/data** â€“ Datasets used in assignments (raw/processed).  
- **/src** â€“ Source code and helper scripts.  
- **/notebooks** â€“ Drafts and exploration notebooks.  
- **/homework** â€“ Final homework submissions (by stage).  

---

## Homework 1 â€“ Stage 1: Problem Framing & Scoping  

### Problem Statement  
Credit risk assessment remains one of the most critical challenges in financial services. Traditional models depend mainly on bureau data and repayment history, which disadvantages borrowers with limited or no credit history. This project explores a more inclusive credit risk modeling framework that combines traditional signals with alternative data sources (e.g., transaction behavior, employment stability, digital footprints) to improve prediction accuracy, fairness, and financial inclusion.  

### Scope of Work  
- Identify limitations of existing credit risk assessment approaches.  
- Propose data sources (traditional + alternative) and justify their relevance.  
- Outline potential modeling approaches (machine learning, interpretable models).  
- Consider fairness, ethical, and regulatory aspects in evaluation.  
- Define clear success criteria: accuracy, fairness, and robustness.  

### Expected Outcomes  
- More accurate credit default prediction compared to baseline bureau-only models.  
- Expanded access to credit for underserved or â€œthin-fileâ€ borrowers.  
- Framework aligned with ethical and regulatory standards.  

## Stage 05 â€“ Data Storage

For this stage, I worked with environment-driven paths (`DATA_DIR_RAW`, `DATA_DIR_PROCESSED`) to save and reload data in multiple formats.

### Files Created
- `data/raw/example.csv`
- `data/raw/example.csv.gz`
- `data/processed/example.parquet`
- `data/processed/example.snappy.parquet`

### Validation
Both CSV and Parquet were reloaded successfully and the data matched exactly (`Alice`, `Bob`, `Charlie`).

### File Sizes (KB)
- CSV: **0.03**
- CSV (gzip): **0.07**
- Parquet: **1.62**
- Parquet (snappy): **1.62**

### Observations
- For very small datasets, CSV may look smaller because Parquet carries schema + metadata overhead.  
- On larger datasets, **Parquet is typically smaller and much faster** for analytics because of its binary, columnar format and built-in compression.  
- CSV is human-readable and simple to share, but inefficient for storage and queries.  
- Parquet is not human-readable, but it is the better choice for scalable storage and analytical workloads.

**Conclusion:**  
Use **CSV** for small / human-readable cases.  
Use **Parquet** for efficient, large-scale data storage and analytics.

## Stage 13 Productization
- API at `api/app.py`
- Model artifacts in `model/`
- Reusable code in `src/`
- Reports in `reports/`
- Deliverables in `deliverables/`

## Stage 13 Productization
- API at `api/app.py`
- Model artifacts in `model/`
- Reusable code in `src/`
- Reports in `reports/`
- Deliverables in `deliverables/`
=======
>>>>>>> origin/main
